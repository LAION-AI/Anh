{
    "debug": False,
    "training": {
        # project name
        "project": "laion-instruct",
        # experiment name
        "exp_name": "xglm-7.5B",
        # Start step. if you want to retrain a model, set the step number of the checkpoint.
        "current_step": 0,
        # Total number of training steps.
        "total_step": 300000,
        # Random seed value.
        "seed": 42,
        # Model save path
        "save_path": "/fsx/ryan/project/laion/Anh/ahn/result",
        # Evaluation interval
        "train_print_every": 10,
        "eval_print_every": 20,
        "eval_interval": 1000,
    },
    "model_and_tokenizer": {
        # Pretrained model name or path.
        "pretrained_model_name": "facebook/xglm-7.5B",
        # Pretrained model type
        "pretrained_model_type": "AutoModelForCausalLM",
        # Pretrained tokenizer name or path.
        "pretrained_tokenizer_name": "facebook/xglm-7.5B",
        # Pretrained tokenizer type
        "pretrained_tokenizer_type": "AutoTokenizer",
        # Model input names
        "model_input_names": ["input_ids", "attention_mask"],
        # Maximum length of input sequence.
        "max_length": 4096,
        # Cache directory for hugging face.
        "cache_dir": "/fsx/ryan/project/laion/Anh/ahn/hf_cache",
        # Low CPU memory usage mode.
        "low_cpu_mem_usage": False,
    },
    "data": {
        # Path to training data.
        "train_data_path": "/fsx/ryan/project/laion/Anh/data/train.jsonl",
        "train_data_key": "text",
        # Path to validation data.
        "valid_data_path": "/fsx/ryan/project/laion/Anh/data/test.jsonl",
        "valid_data_key": "text",
        # Special tokens for the model.
        "special_tokens":[]
    },
    "efficiency": {
        # Whether to use gradient checkpointing to save memory.
        "gradient_checkpointing": True,
        # Whether to fuse activation function to reduce time consumption by elementwise operations.
        "activation_fusion": True,
        # Whether to use smart batching to reduce time consumption by padding.
        "smart_batching": True,
        # Whether to use tensor cores to reduce time consumption by matrix operations.
        # Available only on Ampere GPU (A10, A40, A100, ...).
        "allow_tf32": True,
    },
    "deepspeed": {
        # Training micro batch size per GPU.
        "train_micro_batch_size_per_gpu": 2,
        # Optimizer related settings.
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": 3.0e-5,
                "weight_decay": 3.0e-7,
            },
        },
        # BF16 Mixed precision related settings.
        # Available only on Ampere GPU (A10, A40, A100, ...).
        "bf16": {
            "enable": True,
        },
        # Fp16 Mixed precision related settings.
        "fp16": {
            "enabled": False,
        },
        # "activation_checkpointing": {
        #     # "partition_activations": True,
        #     # "contiguous_memory_optimization": True,
        #     "number_checkpoints": 8,
        # },
        # ZeRO optimization related settings.
        "zero_optimization": {
            "stage": 3,
            # "offload_param": {
            #     "device": "cpu",
            # },
            "offload_optimizer": {
                "device": "cpu",
            },
            "allgather_partitions": True,
            "overlap_comm": True,
            "reduce_scatter": True,
            "contiguous_gradients": True,
        },
        # Whether to allow untested optimizer.
        "zero_allow_untested_optimizer": True,
        # Whether to print wall clock breakdown.
        "wall_clock_breakdown": False,
        # Number of steps to print deepspeed log.
        "steps_per_print": 1000,
    },
}
